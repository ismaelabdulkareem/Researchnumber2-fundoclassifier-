{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GBIZ7xbkqwK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential, models, layers # type: ignore\n",
        "from tensorflow.keras.applications import ResNet50, ResNet101,VGG16,VGG19,DenseNet201,DenseNet121,DenseNet169 # type: ignore\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout # type: ignore\n",
        "from tensorflow.keras.models import Model # type: ignore\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8GhFYeXicEx",
        "outputId": "fabc897a-cf12-4917-b67d-1e733e996e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oXlxcxLlA7L",
        "outputId": "8b2a83ed-4af3-4f8f-9097-48c6623f9f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "data = np.load('/content/drive/MyDrive/datasetWithoutCLAHE_TrainAndTest.npz')\n",
        "\n",
        "train_images = data['train_images']\n",
        "train_labels = data['train_labels']\n",
        "# val_images = data['val_images']\n",
        "# val_labels = data['val_labels']\n",
        "test_images = data['test_images']\n",
        "test_labels = data['test_labels']\n",
        "\n",
        "print(\"Data loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGDevOw2laRL"
      },
      "outputs": [],
      "source": [
        "\n",
        " # default input size for the model\n",
        "def resize_image(image):\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    return image\n",
        "\n",
        " # Normalize bo raingi [0, 1] \n",
        "def normalize_images(image):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = image / 255.0 \n",
        "    return image\n",
        "\n",
        "# Function to augment the image\n",
        "def augment_image(image):\n",
        "\n",
        "    image = tf.image.flip_left_right(image)\n",
        "    image = tf.image.flip_up_down(image)\n",
        "    image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))   # Randomly rotate the image\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)  # Randomly adjust brightness\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)  # Randomly adjust contrast\n",
        "    image = tf.image.resize_with_crop_or_pad(image, 266, 266)  # Zoom in slightly\n",
        "    image = tf.image.random_crop(image, size=[256, 256, 3])\n",
        "\n",
        "    # Clip the image to ensure values are in the range [0, 1]\n",
        "    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=0.04) #  (equivalent to ±10/255 on unnormalized)\n",
        "\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(image, label, data_augmentation=True):\n",
        "\n",
        "    image = resize_image(image) # resize the image\n",
        "\n",
        "    image = normalize_images(image)\n",
        "\n",
        "    # (on normalized data)\n",
        "    if data_augmentation: \n",
        "        image = augment_image(image)\n",
        "\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "batch_size=32  \n",
        "classes=4\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices( (train_images , train_labels) )\n",
        "\n",
        "# shuffling individual samples\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(train_images))\n",
        "train_dataset = train_dataset.map(lambda x, y: load_and_preprocess_image(x, y, data_augmentation=True), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.prefetch(buffer_size= tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices( (test_images , test_labels) )\n",
        "test_dataset = test_dataset.map(lambda x, y: load_and_preprocess_image(x, y, data_augmentation=False), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrO6Vp9JmKNc"
      },
      "outputs": [],
      "source": [
        "# Load ResNet50 with pre-trained weights, exclude the top layers\n",
        "base_model = DenseNet169(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "# Freeze the base model to prevent retraining\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLzsMtW9qiMS"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(base_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(4, activation='softmax'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NGNfROXqn6R"
      },
      "outputs": [],
      "source": [
        "# Define the callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',  # Monitoring validation accuracy\n",
        "    patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,\n",
        "    mode='max',  # Stops training when the quantity monitored has stopped increasing\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity)\n",
        ")\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/DenseNet169/Ws/epoch_{epoch:02d}.weights.h5',  # Save file with epoch number\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=False\n",
        ")\n",
        "\n",
        "# Add callbacks to the fit method\n",
        "callbacks = [early_stopping, model_checkpoint]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcrP4Or4qp7A"
      },
      "outputs": [],
      "source": [
        "input_shape = (224, 224, 3)  # Modify as per your model's input dimensions\n",
        "model.build(input_shape=(None, *input_shape))\n",
        "# Assuming you have defined your model architecture as `model`\n",
        "try:\n",
        "    model.load_weights('/content/drive/MyDrive/DenseNet169/Ws/epoch_18.weights.h5')\n",
        "except ValueError as e:\n",
        "    print(f\"Error loading weights: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0DTyzvPsSrZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam # type: ignore\n",
        "\n",
        "optimizer = Adam(learning_rate=0.000005)  # lower value 0.00001 or higher like 0.001\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G61Dn3umsbnP",
        "outputId": "f14a1d8e-b351-4c2f-d7bb-afd658dac235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.4479 - loss: 1.3833\n",
            "Epoch 1: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_01.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 823ms/step - accuracy: 0.4489 - loss: 1.3806 - val_accuracy: 0.6882 - val_loss: 0.8444\n",
            "Epoch 2/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.6657 - loss: 0.8133\n",
            "Epoch 2: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_02.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 449ms/step - accuracy: 0.6659 - loss: 0.8130 - val_accuracy: 0.7517 - val_loss: 0.6482\n",
            "Epoch 3/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7296 - loss: 0.6858\n",
            "Epoch 3: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_03.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 575ms/step - accuracy: 0.7297 - loss: 0.6858 - val_accuracy: 0.8104 - val_loss: 0.5498\n",
            "Epoch 4/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.7633 - loss: 0.6065\n",
            "Epoch 4: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_04.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 565ms/step - accuracy: 0.7633 - loss: 0.6064 - val_accuracy: 0.8303 - val_loss: 0.4831\n",
            "Epoch 5/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7868 - loss: 0.5630\n",
            "Epoch 5: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_05.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 557ms/step - accuracy: 0.7867 - loss: 0.5629 - val_accuracy: 0.8493 - val_loss: 0.4362\n",
            "Epoch 6/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7992 - loss: 0.5100\n",
            "Epoch 6: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_06.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 547ms/step - accuracy: 0.7993 - loss: 0.5099 - val_accuracy: 0.8597 - val_loss: 0.4018\n",
            "Epoch 7/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8129 - loss: 0.4978\n",
            "Epoch 7: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_07.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 519ms/step - accuracy: 0.8129 - loss: 0.4977 - val_accuracy: 0.8626 - val_loss: 0.3875\n",
            "Epoch 8/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8267 - loss: 0.4562\n",
            "Epoch 8: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_08.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 549ms/step - accuracy: 0.8267 - loss: 0.4562 - val_accuracy: 0.8758 - val_loss: 0.3636\n",
            "Epoch 9/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8386 - loss: 0.4261\n",
            "Epoch 9: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_09.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 555ms/step - accuracy: 0.8385 - loss: 0.4263 - val_accuracy: 0.8796 - val_loss: 0.3519\n",
            "Epoch 10/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8379 - loss: 0.4299\n",
            "Epoch 10: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_10.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 626ms/step - accuracy: 0.8379 - loss: 0.4298 - val_accuracy: 0.8768 - val_loss: 0.3442\n",
            "Epoch 11/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.8446 - loss: 0.4076\n",
            "Epoch 11: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_11.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 605ms/step - accuracy: 0.8447 - loss: 0.4076 - val_accuracy: 0.8853 - val_loss: 0.3346\n",
            "Epoch 12/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8473 - loss: 0.4152\n",
            "Epoch 12: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_12.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 561ms/step - accuracy: 0.8473 - loss: 0.4150 - val_accuracy: 0.8872 - val_loss: 0.3296\n",
            "Epoch 13/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8664 - loss: 0.3689\n",
            "Epoch 13: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_13.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 386ms/step - accuracy: 0.8663 - loss: 0.3690 - val_accuracy: 0.8900 - val_loss: 0.3141\n",
            "Epoch 14/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8633 - loss: 0.3767\n",
            "Epoch 14: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_14.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 628ms/step - accuracy: 0.8633 - loss: 0.3766 - val_accuracy: 0.8929 - val_loss: 0.3094\n",
            "Epoch 15/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8588 - loss: 0.3619\n",
            "Epoch 15: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_15.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 579ms/step - accuracy: 0.8588 - loss: 0.3619 - val_accuracy: 0.8938 - val_loss: 0.3050\n",
            "Epoch 16/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8871 - loss: 0.3287\n",
            "Epoch 16: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_16.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 584ms/step - accuracy: 0.8869 - loss: 0.3289 - val_accuracy: 0.8957 - val_loss: 0.3029\n",
            "Epoch 17/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8790 - loss: 0.3441\n",
            "Epoch 17: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_17.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 537ms/step - accuracy: 0.8790 - loss: 0.3441 - val_accuracy: 0.8938 - val_loss: 0.3035\n",
            "Epoch 18/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8735 - loss: 0.3441\n",
            "Epoch 18: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_18.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 372ms/step - accuracy: 0.8735 - loss: 0.3441 - val_accuracy: 0.8957 - val_loss: 0.2911\n",
            "Epoch 19/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8906 - loss: 0.3175\n",
            "Epoch 19: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_19.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 530ms/step - accuracy: 0.8906 - loss: 0.3175 - val_accuracy: 0.8976 - val_loss: 0.2925\n",
            "Epoch 20/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8885 - loss: 0.3062\n",
            "Epoch 20: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_20.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 569ms/step - accuracy: 0.8885 - loss: 0.3063 - val_accuracy: 0.8995 - val_loss: 0.2876\n",
            "Epoch 21/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8927 - loss: 0.2993\n",
            "Epoch 21: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_21.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 490ms/step - accuracy: 0.8927 - loss: 0.2993 - val_accuracy: 0.8957 - val_loss: 0.2790\n",
            "Epoch 22/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.8966 - loss: 0.2903\n",
            "Epoch 22: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_22.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 551ms/step - accuracy: 0.8966 - loss: 0.2903 - val_accuracy: 0.9081 - val_loss: 0.2820\n",
            "Epoch 23/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8968 - loss: 0.2876\n",
            "Epoch 23: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_23.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 563ms/step - accuracy: 0.8968 - loss: 0.2876 - val_accuracy: 0.9043 - val_loss: 0.2759\n",
            "Epoch 24/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9040 - loss: 0.2894\n",
            "Epoch 24: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_24.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 529ms/step - accuracy: 0.9039 - loss: 0.2894 - val_accuracy: 0.9024 - val_loss: 0.2763\n",
            "Epoch 25/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8955 - loss: 0.2892\n",
            "Epoch 25: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_25.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 344ms/step - accuracy: 0.8955 - loss: 0.2893 - val_accuracy: 0.8995 - val_loss: 0.2782\n",
            "Epoch 26/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9085 - loss: 0.2656\n",
            "Epoch 26: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_26.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 347ms/step - accuracy: 0.9084 - loss: 0.2657 - val_accuracy: 0.9052 - val_loss: 0.2715\n",
            "Epoch 27/50\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9011 - loss: 0.2738\n",
            "Epoch 27: saving model to /content/drive/MyDrive/DenseNet169/Ws/epoch_27.weights.h5\n",
            "\u001b[1m99/99\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 321ms/step - accuracy: 0.9011 - loss: 0.2738 - val_accuracy: 0.9081 - val_loss: 0.2677\n",
            "Epoch 27: early stopping\n",
            "Restoring model weights from the end of the best epoch: 22.\n"
          ]
        }
      ],
      "source": [
        "# Continue training for the remaining epochs\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    #initial_epoch=18,  # Set this to the epoch number you finished on\n",
        "    epochs=50,  # Continue for the total desired epochs\n",
        "    callbacks=[model_checkpoint, early_stopping]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "C01RfPIUM7ol",
        "outputId": "8c2653d3-636e-4c20-82e4-e905a92e5623"
      },
      "outputs": [],
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.plot(history.history['accuracy'], color=\"#E74C3C\", marker='o')\n",
        "plt.plot(history.history['val_accuracy'], color='#641E16', marker='h')\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.grid(True)\n",
        "#plt.savefig('C:/Users/PC_I/OneDrive/Desktop/accuracy_plot.png')  # Save the accuracy plot as a PNG file\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.plot(history.history['loss'], color=\"#E74C3C\", marker='o')\n",
        "plt.plot(history.history['val_loss'], color='#641E16', marker='h')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.grid(True)\n",
        "#plt.savefig('C:/Users/PC_I/OneDrive/Desktop/loss_plot.png')  # Save the loss plot as a PNG file\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu1KdjpuNdH4",
        "outputId": "9c3d9baf-abed-46cb-cb02-dc5ed08b2909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.908285915851593\n",
            "Validation Accuracy: 0.8872038125991821\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on training data\n",
        "train_loss, train_accuracy = model.evaluate(train_dataset, verbose=0)\n",
        "print(f\"Training Accuracy: {train_accuracy}\")\n",
        "\n",
        "# Evaluate on validation data\n",
        "val_loss, val_accuracy = model.evaluate(test_dataset, verbose=0)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gPDOKSOBsf",
        "outputId": "f9bd194d-8d7c-41a3-cb8a-8c1e2696c105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 574ms/step\n",
            "Class Names: Cataract, Glaucoma, Normal, DR\n",
            "\n",
            "Confusion Matrix:\n",
            " [[240  14   5   0]\n",
            " [ 22 187  41   2]\n",
            " [  7  27 235   0]\n",
            " [  0   1   0 274]]\n",
            "\n",
            "Classification Report for Classes: Cataract, Glaucoma, Normal, DR\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Cataract       0.89      0.93      0.91       259\n",
            "    Glaucoma       0.82      0.74      0.78       252\n",
            "      Normal       0.84      0.87      0.85       269\n",
            "          DR       0.99      1.00      0.99       275\n",
            "\n",
            "    accuracy                           0.89      1055\n",
            "   macro avg       0.88      0.88      0.88      1055\n",
            "weighted avg       0.89      0.89      0.89      1055\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load your saved model (e.g., model from the last epoch)\n",
        "# try:\n",
        "#     model.load_weights('C:/Users/PC_I/OneDrive/Desktop/EDC/model_epoch_78.weights.h5')\n",
        "# except ValueError as e:\n",
        "#     print(f\"Error loading weights: {e}\")\n",
        "\n",
        "# Step 2: Make predictions on the validation dataset\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Iterate through the validation dataset and collect predictions and true labels\n",
        "for images, labels in test_dataset:  # Assuming val_dataset is already batched\n",
        "    predictions = model.predict(images)\n",
        "\n",
        "    # Append the true labels directly if they are integer-encoded\n",
        "    y_true.extend(labels.numpy())  # Convert TensorFlow tensor to numpy array\n",
        "\n",
        "    # Append the predicted labels (taking the argmax of the predictions)\n",
        "    y_pred.extend(np.argmax(predictions, axis=1))  # Predicted labels\n",
        "\n",
        "# Step 3: Define class names\n",
        "class_names = ['Cataract', 'Glaucoma', 'Normal', 'DR']  # Adjust based on your classes\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Print the class names above the confusion matrix\n",
        "print(f\"Class Names: {', '.join(class_names)}\\n\")\n",
        "\n",
        "# Print confusion matrix with class labels\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Classification report (includes precision, recall, f1-score for each class)\n",
        "report = classification_report(y_true, y_pred, target_names=class_names)\n",
        "\n",
        "# Print the class names above the classification report\n",
        "print(f\"\\nClassification Report for Classes: {', '.join(class_names)}\")\n",
        "print(\"\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0k2qtpeOGZV",
        "outputId": "3ca6a8f7-8dd3-4c41-d1f1-df0580a81d12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for class Cataract: 0.93\n",
            "Accuracy for class Glaucoma: 0.74\n",
            "Accuracy for class Normal: 0.87\n",
            "Accuracy for class DR: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Calculate per-class accuracy from the confusion matrix\n",
        "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "for idx, accuracy in enumerate(per_class_accuracy):\n",
        "    print(f\"Accuracy for class {class_names[idx]}: {accuracy:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
